{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with PyTorch\n",
    "We're going to train a neural network to classify dogs and cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init, helpers, utils, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ppt import utils\n",
    "from ppt.utils import attr\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# list(get_trainable(model.parameters()))\n",
    "# list(get_frozen(model.parameters()))\n",
    "# all_trainable(model.parameters())\n",
    "# all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224\n",
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize(256),  # some images are pretty small\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/dogscats/train.\n",
      "Loading data from ../data/raw/dogscats/valid.\n"
     ]
    }
   ],
   "source": [
    "from ppt.utils import DogsCatsDataset\n",
    "\n",
    "train_ds = DogsCatsDataset(\"../data/raw\", \"train\", transform=train_trans)\n",
    "val_ds = DogsCatsDataset(\"../data/raw\", \"valid\", transform=val_trans)\n",
    "# train_ds = DogsCatsDataset(\"../data/raw\", \"sample/train\", transform=train_trans)\n",
    "# val_ds = DogsCatsDataset(\"../data/raw\", \"sample/valid\", transform=val_trans)\n",
    "\n",
    "\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23000, 2000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Batch loading for datasets with multi-processing and different sample strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "# BATCH_SIZE = 512\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "PyTorch offers quite a few [pre-trained networks](https://pytorch.org/docs/stable/torchvision/models.html) for you to use:\n",
    "- AlexNet\n",
    "- VGG\n",
    "- ResNet\n",
    "- SqueezeNet\n",
    "- DenseNet\n",
    "- Inception v3\n",
    "\n",
    "And there are more available via [pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch)\n",
    "- NASNet,\n",
    "- ResNeXt,\n",
    "- InceptionV4,\n",
    "- InceptionResnetV2, \n",
    "- Xception, \n",
    "- DPN,\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_all(model.parameters())\n",
    "assert all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the last layer with a linear layer. New layers have `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_classes=2):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    freeze_all(model.parameters())\n",
    "    model.fc = nn.Linear(512, n_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    get_trainable(model.parameters()),  # model.fc.parameters()\n",
    "    lr=0.001,\n",
    "    # momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "    batch loss: 0.842\n",
      "    batch loss: 0.742\n",
      "    batch loss: 0.677\n",
      "    batch loss: 0.644\n",
      "    batch loss: 0.637\n",
      "    batch loss: 0.536\n",
      "    batch loss: 0.494\n",
      "    batch loss: 0.476\n",
      "    batch loss: 0.479\n",
      "    batch loss: 0.426\n",
      "    batch loss: 0.425\n",
      "    batch loss: 0.363\n",
      "    batch loss: 0.380\n",
      "    batch loss: 0.341\n",
      "    batch loss: 0.350\n",
      "    batch loss: 0.310\n",
      "    batch loss: 0.384\n",
      "    batch loss: 0.260\n",
      "    batch loss: 0.308\n",
      "    batch loss: 0.252\n",
      "    batch loss: 0.281\n",
      "    batch loss: 0.273\n",
      "    batch loss: 0.236\n",
      "    batch loss: 0.222\n",
      "    batch loss: 0.169\n",
      "    batch loss: 0.238\n",
      "    batch loss: 0.216\n",
      "    batch loss: 0.229\n",
      "    batch loss: 0.235\n",
      "    batch loss: 0.189\n",
      "    batch loss: 0.214\n",
      "    batch loss: 0.202\n",
      "    batch loss: 0.263\n",
      "    batch loss: 0.154\n",
      "    batch loss: 0.193\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.184\n",
      "    batch loss: 0.197\n",
      "    batch loss: 0.158\n",
      "    batch loss: 0.178\n",
      "    batch loss: 0.187\n",
      "    batch loss: 0.162\n",
      "    batch loss: 0.171\n",
      "    batch loss: 0.171\n",
      "    batch loss: 0.174\n",
      "    batch loss: 0.159\n",
      "    batch loss: 0.180\n",
      "    batch loss: 0.179\n",
      "    batch loss: 0.156\n",
      "    batch loss: 0.154\n",
      "    batch loss: 0.185\n",
      "    batch loss: 0.146\n",
      "    batch loss: 0.120\n",
      "    batch loss: 0.126\n",
      "    batch loss: 0.130\n",
      "    batch loss: 0.144\n",
      "    batch loss: 0.144\n",
      "    batch loss: 0.097\n",
      "    batch loss: 0.139\n",
      "    batch loss: 0.124\n",
      "    batch loss: 0.141\n",
      "    batch loss: 0.136\n",
      "    batch loss: 0.143\n",
      "    batch loss: 0.141\n",
      "    batch loss: 0.113\n",
      "    batch loss: 0.114\n",
      "    batch loss: 0.186\n",
      "    batch loss: 0.142\n",
      "    batch loss: 0.124\n",
      "    batch loss: 0.119\n",
      "    batch loss: 0.122\n",
      "    batch loss: 0.145\n",
      "    batch loss: 0.121\n",
      "    batch loss: 0.139\n",
      "    batch loss: 0.116\n",
      "    batch loss: 0.100\n",
      "    batch loss: 0.109\n",
      "    batch loss: 0.090\n",
      "    batch loss: 0.070\n",
      "    batch loss: 0.134\n",
      "    batch loss: 0.093\n",
      "    batch loss: 0.124\n",
      "    batch loss: 0.158\n",
      "    batch loss: 0.090\n",
      "    batch loss: 0.142\n",
      "    batch loss: 0.120\n",
      "    batch loss: 0.091\n",
      "    batch loss: 0.130\n",
      "    batch loss: 0.101\n",
      "    batch loss: 0.161\n",
      "    batch loss: 0.116\n",
      "    batch loss: 0.146\n",
      "    batch loss: 0.089\n",
      "    batch loss: 0.092\n",
      "    batch loss: 0.121\n",
      "    batch loss: 0.127\n",
      "    batch loss: 0.085\n",
      "    batch loss: 0.114\n",
      "    batch loss: 0.083\n",
      "    batch loss: 0.116\n",
      "    batch loss: 0.113\n",
      "    batch loss: 0.137\n",
      "    batch loss: 0.121\n",
      "    batch loss: 0.129\n",
      "    batch loss: 0.093\n",
      "    batch loss: 0.113\n",
      "    batch loss: 0.095\n",
      "    batch loss: 0.108\n",
      "    batch loss: 0.146\n",
      "    batch loss: 0.059\n",
      "    batch loss: 0.092\n",
      "    batch loss: 0.123\n",
      "    batch loss: 0.099\n",
      "    batch loss: 0.106\n",
      "    batch loss: 0.092\n",
      "    batch loss: 0.108\n",
      "    batch loss: 0.109\n",
      "    batch loss: 0.104\n",
      "    batch loss: 0.106\n",
      "    batch loss: 0.103\n",
      "    batch loss: 0.112\n",
      "    batch loss: 0.108\n",
      "    batch loss: 0.105\n",
      "    batch loss: 0.114\n",
      "    batch loss: 0.055\n",
      "    batch loss: 0.110\n",
      "    batch loss: 0.113\n",
      "    batch loss: 0.100\n",
      "    batch loss: 0.113\n",
      "    batch loss: 0.113\n",
      "    batch loss: 0.081\n",
      "    batch loss: 0.120\n",
      "    batch loss: 0.081\n",
      "    batch loss: 0.074\n",
      "    batch loss: 0.117\n",
      "    batch loss: 0.122\n",
      "    batch loss: 0.131\n",
      "    batch loss: 0.200\n",
      "    batch loss: 0.077\n",
      "    batch loss: 0.085\n",
      "    batch loss: 0.097\n",
      "    batch loss: 0.053\n",
      "    batch loss: 0.124\n",
      "    batch loss: 0.088\n",
      "    batch loss: 0.080\n",
      "    batch loss: 0.083\n",
      "    batch loss: 0.100\n",
      "    batch loss: 0.057\n",
      "    batch loss: 0.081\n",
      "    batch loss: 0.128\n",
      "    batch loss: 0.101\n",
      "    batch loss: 0.079\n",
      "    batch loss: 0.142\n",
      "    batch loss: 0.161\n",
      "    batch loss: 0.104\n",
      "    batch loss: 0.092\n",
      "    batch loss: 0.112\n",
      "    batch loss: 0.124\n",
      "    batch loss: 0.089\n",
      "    batch loss: 0.106\n",
      "    batch loss: 0.127\n",
      "    batch loss: 0.122\n",
      "    batch loss: 0.069\n",
      "    batch loss: 0.095\n",
      "    batch loss: 0.078\n",
      "    batch loss: 0.084\n",
      "    batch loss: 0.081\n",
      "    batch loss: 0.102\n",
      "    batch loss: 0.158\n",
      "    batch loss: 0.083\n",
      "    batch loss: 0.151\n",
      "    batch loss: 0.058\n",
      "    batch loss: 0.058\n",
      "    batch loss: 0.092\n",
      "    batch loss: 0.059\n",
      "    batch loss: 0.094\n",
      "    batch loss: 0.076\n",
      "    batch loss: 0.083\n",
      "    batch loss: 0.057\n",
      "    batch loss: 0.098\n",
      "  Train Loss: 0.16412624974354453\n",
      "  Train Acc:  0.9405217391304348\n",
      "  Valid Loss: 0.07101822155714035\n",
      "  Valid Acc:  0.9765\n",
      "\n",
      "Epoch 2/2\n",
      "    batch loss: 0.091\n",
      "    batch loss: 0.082\n",
      "    batch loss: 0.082\n",
      "    batch loss: 0.103\n",
      "    batch loss: 0.085\n",
      "    batch loss: 0.178\n",
      "    batch loss: 0.130\n",
      "    batch loss: 0.073\n",
      "    batch loss: 0.086\n",
      "    batch loss: 0.098\n",
      "    batch loss: 0.056\n",
      "    batch loss: 0.115\n",
      "    batch loss: 0.077\n",
      "    batch loss: 0.074\n",
      "    batch loss: 0.122\n",
      "    batch loss: 0.091\n",
      "    batch loss: 0.101\n",
      "    batch loss: 0.071\n",
      "    batch loss: 0.134\n",
      "    batch loss: 0.097\n",
      "    batch loss: 0.081\n",
      "    batch loss: 0.119\n",
      "    batch loss: 0.103\n",
      "    batch loss: 0.075\n",
      "    batch loss: 0.075\n",
      "    batch loss: 0.067\n",
      "    batch loss: 0.100\n",
      "    batch loss: 0.080\n",
      "    batch loss: 0.049\n",
      "    batch loss: 0.072\n",
      "    batch loss: 0.186\n",
      "    batch loss: 0.063\n",
      "    batch loss: 0.063\n",
      "    batch loss: 0.104\n",
      "    batch loss: 0.102\n",
      "    batch loss: 0.120\n",
      "    batch loss: 0.048\n",
      "    batch loss: 0.065\n",
      "    batch loss: 0.063\n",
      "    batch loss: 0.071\n",
      "    batch loss: 0.058\n",
      "    batch loss: 0.052\n",
      "    batch loss: 0.074\n",
      "    batch loss: 0.125\n",
      "    batch loss: 0.084\n",
      "    batch loss: 0.151\n",
      "    batch loss: 0.084\n",
      "    batch loss: 0.091\n",
      "    batch loss: 0.077\n",
      "    batch loss: 0.097\n",
      "    batch loss: 0.106\n",
      "    batch loss: 0.066\n",
      "    batch loss: 0.128\n",
      "    batch loss: 0.092\n",
      "    batch loss: 0.094\n",
      "    batch loss: 0.099\n",
      "    batch loss: 0.090\n",
      "    batch loss: 0.104\n",
      "    batch loss: 0.114\n",
      "    batch loss: 0.098\n",
      "    batch loss: 0.085\n",
      "    batch loss: 0.124\n",
      "    batch loss: 0.093\n",
      "    batch loss: 0.086\n",
      "    batch loss: 0.056\n",
      "    batch loss: 0.086\n",
      "    batch loss: 0.088\n",
      "    batch loss: 0.072\n",
      "    batch loss: 0.108\n",
      "    batch loss: 0.149\n",
      "    batch loss: 0.134\n",
      "    batch loss: 0.091\n",
      "    batch loss: 0.078\n",
      "    batch loss: 0.073\n",
      "    batch loss: 0.073\n",
      "    batch loss: 0.151\n",
      "    batch loss: 0.111\n",
      "    batch loss: 0.117\n",
      "    batch loss: 0.126\n",
      "    batch loss: 0.084\n",
      "    batch loss: 0.067\n",
      "    batch loss: 0.058\n",
      "    batch loss: 0.109\n",
      "    batch loss: 0.050\n",
      "    batch loss: 0.170\n",
      "    batch loss: 0.083\n",
      "    batch loss: 0.073\n",
      "    batch loss: 0.093\n",
      "    batch loss: 0.082\n",
      "    batch loss: 0.068\n",
      "    batch loss: 0.094\n",
      "    batch loss: 0.095\n",
      "    batch loss: 0.124\n",
      "    batch loss: 0.085\n",
      "    batch loss: 0.049\n",
      "    batch loss: 0.079\n",
      "    batch loss: 0.091\n",
      "    batch loss: 0.073\n",
      "    batch loss: 0.072\n",
      "    batch loss: 0.067\n",
      "    batch loss: 0.079\n",
      "    batch loss: 0.048\n",
      "    batch loss: 0.209\n",
      "    batch loss: 0.120\n",
      "    batch loss: 0.098\n",
      "    batch loss: 0.055\n",
      "    batch loss: 0.068\n",
      "    batch loss: 0.103\n",
      "    batch loss: 0.079\n",
      "    batch loss: 0.089\n",
      "    batch loss: 0.079\n",
      "    batch loss: 0.082\n",
      "    batch loss: 0.090\n",
      "    batch loss: 0.046\n",
      "    batch loss: 0.107\n",
      "    batch loss: 0.059\n",
      "    batch loss: 0.082\n",
      "    batch loss: 0.076\n",
      "    batch loss: 0.049\n",
      "    batch loss: 0.055\n",
      "    batch loss: 0.093\n",
      "    batch loss: 0.140\n",
      "    batch loss: 0.085\n",
      "    batch loss: 0.081\n",
      "    batch loss: 0.049\n",
      "    batch loss: 0.060\n",
      "    batch loss: 0.070\n",
      "    batch loss: 0.079\n",
      "    batch loss: 0.049\n",
      "    batch loss: 0.087\n",
      "    batch loss: 0.100\n",
      "    batch loss: 0.115\n",
      "    batch loss: 0.124\n",
      "    batch loss: 0.087\n",
      "    batch loss: 0.109\n",
      "    batch loss: 0.040\n",
      "    batch loss: 0.113\n",
      "    batch loss: 0.068\n",
      "    batch loss: 0.083\n",
      "    batch loss: 0.057\n",
      "    batch loss: 0.108\n",
      "    batch loss: 0.070\n",
      "    batch loss: 0.098\n",
      "    batch loss: 0.101\n",
      "    batch loss: 0.049\n",
      "    batch loss: 0.064\n",
      "    batch loss: 0.098\n",
      "    batch loss: 0.051\n",
      "    batch loss: 0.107\n",
      "    batch loss: 0.074\n",
      "    batch loss: 0.155\n",
      "    batch loss: 0.064\n",
      "    batch loss: 0.053\n",
      "    batch loss: 0.067\n",
      "    batch loss: 0.077\n",
      "    batch loss: 0.046\n",
      "    batch loss: 0.094\n",
      "    batch loss: 0.066\n",
      "    batch loss: 0.069\n",
      "    batch loss: 0.043\n",
      "    batch loss: 0.059\n",
      "    batch loss: 0.060\n",
      "    batch loss: 0.095\n",
      "    batch loss: 0.139\n",
      "    batch loss: 0.027\n",
      "    batch loss: 0.063\n",
      "    batch loss: 0.090\n",
      "    batch loss: 0.099\n",
      "    batch loss: 0.039\n",
      "    batch loss: 0.098\n",
      "    batch loss: 0.088\n",
      "    batch loss: 0.045\n",
      "    batch loss: 0.058\n",
      "    batch loss: 0.077\n",
      "    batch loss: 0.072\n",
      "    batch loss: 0.069\n",
      "    batch loss: 0.051\n",
      "    batch loss: 0.088\n",
      "    batch loss: 0.098\n",
      "    batch loss: 0.066\n",
      "  Train Loss: 0.08672944898968157\n",
      "  Train Acc:  0.9679130434782609\n",
      "  Valid Loss: 0.05658263339102268\n",
      "  Valid Acc:  0.978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    model.train()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # with torch.set_grad_enabled(True):\n",
    "        y_ = model(X)\n",
    "        loss = criterion(y_, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        print(f\"    batch loss: {loss.item():0.3f}\")\n",
    "        _, y_label_ = torch.max(y_, 1)\n",
    "        correct += (y_label_ == y).sum().item()\n",
    "        running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Train Loss: {running_loss / len(train_dl.dataset)}\")\n",
    "    print(f\"  Train Acc:  {correct / len(train_dl.dataset)}\")\n",
    "    \n",
    "    \n",
    "    # Eval\n",
    "    model.eval()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    with torch.no_grad():  # IMPORTANT\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "                    \n",
    "            y_ = model(X)\n",
    "        \n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            correct += (y_label_ == y).sum().item()\n",
    "            \n",
    "            loss = criterion(y_, y)\n",
    "            running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Valid Loss: {running_loss / len(val_dl.dataset)}\")\n",
    "    print(f\"  Valid Acc:  {correct / len(val_dl.dataset)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermission: training libraries\n",
    "\n",
    "Writing the training loop is my least favourite thing about PyTorch.\n",
    "\n",
    "Keras is great here!\n",
    "```python\n",
    "model.compile(optimizer, criterion, metrics=[\"accuracy\", \"f1\"])\n",
    "model.fit(X, y, epochs=10)\n",
    "```\n",
    "\n",
    "\n",
    "### [Ignite](https://github.com/pytorch/ignite)\n",
    "> Ignite is a high-level library to help with training neural networks in PyTorch.\n",
    "> - ignite helps you write compact but full-featured training loops in a few lines of code\n",
    "> - you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n",
    "\n",
    "\n",
    "### [TNT](https://github.com/pytorch/tnt)\n",
    "> TNT is a library providing powerful dataloading, logging and visualization utlities for Python. It is closely intergrated with PyTorch and is designed to enable rapid iteration with any model or training regimen.\n",
    "> [...]\n",
    "> The project was inspired by TorchNet, and legend says that it stood for “TorchNetTwo”\n",
    "\n",
    "\n",
    "### [Skorch](https://github.com/dnouri/skorch)\n",
    "> A scikit-learn compatible neural network library that wraps PyTorch.\n",
    "\n",
    "\n",
    "### \"The fun of Reinvention\"\n",
    "Clearly, there must be a better way! Write your own lib (but don't use it) :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import (\n",
    "    CategoricalAccuracy,\n",
    "    Loss,\n",
    "    Precision,\n",
    ")\n",
    "from ignite.engine import (\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    "    Events,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, loss, optimizer\n",
    "model = get_model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    get_trainable(model.parameters()),\n",
    "    lr=0.001,\n",
    "    momentum=.9,\n",
    ")\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": CategoricalAccuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "        \"precision\": Precision(),\n",
    "    },\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}] Batch[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "\n",
    "# trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Batch[1] Loss: 0.75\n",
      "Epoch[1] Batch[2] Loss: 0.68\n",
      "Epoch[1] Batch[3] Loss: 0.68\n",
      "Epoch[1] Batch[4] Loss: 0.61\n",
      "Epoch[1] Batch[5] Loss: 0.69\n",
      "Epoch[1] Batch[6] Loss: 0.62\n",
      "Epoch[1] Batch[7] Loss: 0.62\n",
      "Epoch[1] Batch[8] Loss: 0.57\n",
      "Epoch[1] Batch[9] Loss: 0.55\n",
      "Epoch[1] Batch[10] Loss: 0.50\n",
      "Epoch[1] Batch[11] Loss: 0.45\n",
      "Epoch[1] Batch[12] Loss: 0.47\n",
      "Epoch[1] Batch[13] Loss: 0.40\n",
      "Epoch[1] Batch[14] Loss: 0.42\n",
      "Epoch[1] Batch[15] Loss: 0.35\n",
      "Epoch[1] Batch[16] Loss: 0.33\n",
      "Epoch[1] Batch[17] Loss: 0.30\n",
      "Epoch[1] Batch[18] Loss: 0.29\n",
      "Epoch[1] Batch[19] Loss: 0.27\n",
      "Epoch[1] Batch[20] Loss: 0.29\n",
      "Epoch[1] Batch[21] Loss: 0.28\n",
      "Epoch[1] Batch[22] Loss: 0.27\n",
      "Epoch[1] Batch[23] Loss: 0.27\n",
      "Epoch[1] Batch[24] Loss: 0.27\n",
      "Epoch[1] Batch[25] Loss: 0.25\n",
      "Epoch[1] Batch[26] Loss: 0.22\n",
      "Epoch[1] Batch[27] Loss: 0.22\n",
      "Epoch[1] Batch[28] Loss: 0.22\n",
      "Epoch[1] Batch[29] Loss: 0.24\n",
      "Epoch[1] Batch[30] Loss: 0.19\n",
      "Epoch[1] Batch[31] Loss: 0.17\n",
      "Epoch[1] Batch[32] Loss: 0.23\n",
      "Epoch[1] Batch[33] Loss: 0.17\n",
      "Epoch[1] Batch[34] Loss: 0.18\n",
      "Epoch[1] Batch[35] Loss: 0.21\n",
      "Epoch[1] Batch[36] Loss: 0.19\n",
      "Epoch[1] Batch[37] Loss: 0.16\n",
      "Epoch[1] Batch[38] Loss: 0.18\n",
      "Epoch[1] Batch[39] Loss: 0.17\n",
      "Epoch[1] Batch[40] Loss: 0.11\n",
      "Epoch[1] Batch[41] Loss: 0.16\n",
      "Epoch[1] Batch[42] Loss: 0.16\n",
      "Epoch[1] Batch[43] Loss: 0.19\n",
      "Epoch[1] Batch[44] Loss: 0.15\n",
      "Epoch[1] Batch[45] Loss: 0.15\n",
      "Epoch[1] Batch[46] Loss: 0.19\n",
      "Epoch[1] Batch[47] Loss: 0.16\n",
      "Epoch[1] Batch[48] Loss: 0.15\n",
      "Epoch[1] Batch[49] Loss: 0.15\n",
      "Epoch[1] Batch[50] Loss: 0.17\n",
      "Epoch[1] Batch[51] Loss: 0.15\n",
      "Epoch[1] Batch[52] Loss: 0.17\n",
      "Epoch[1] Batch[53] Loss: 0.10\n",
      "Epoch[1] Batch[54] Loss: 0.17\n",
      "Epoch[1] Batch[55] Loss: 0.16\n",
      "Epoch[1] Batch[56] Loss: 0.14\n",
      "Epoch[1] Batch[57] Loss: 0.13\n",
      "Epoch[1] Batch[58] Loss: 0.14\n",
      "Epoch[1] Batch[59] Loss: 0.14\n",
      "Epoch[1] Batch[60] Loss: 0.13\n",
      "Epoch[1] Batch[61] Loss: 0.11\n",
      "Epoch[1] Batch[62] Loss: 0.18\n",
      "Epoch[1] Batch[63] Loss: 0.11\n",
      "Epoch[1] Batch[64] Loss: 0.14\n",
      "Epoch[1] Batch[65] Loss: 0.14\n",
      "Epoch[1] Batch[66] Loss: 0.15\n",
      "Epoch[1] Batch[67] Loss: 0.13\n",
      "Epoch[1] Batch[68] Loss: 0.17\n",
      "Epoch[1] Batch[69] Loss: 0.14\n",
      "Epoch[1] Batch[70] Loss: 0.15\n",
      "Epoch[1] Batch[71] Loss: 0.14\n",
      "Epoch[1] Batch[72] Loss: 0.13\n",
      "Epoch[1] Batch[73] Loss: 0.14\n",
      "Epoch[1] Batch[74] Loss: 0.12\n",
      "Epoch[1] Batch[75] Loss: 0.15\n",
      "Epoch[1] Batch[76] Loss: 0.12\n",
      "Epoch[1] Batch[77] Loss: 0.12\n",
      "Epoch[1] Batch[78] Loss: 0.11\n",
      "Epoch[1] Batch[79] Loss: 0.11\n",
      "Epoch[1] Batch[80] Loss: 0.15\n",
      "Epoch[1] Batch[81] Loss: 0.14\n",
      "Epoch[1] Batch[82] Loss: 0.12\n",
      "Epoch[1] Batch[83] Loss: 0.10\n",
      "Epoch[1] Batch[84] Loss: 0.16\n",
      "Epoch[1] Batch[85] Loss: 0.18\n",
      "Epoch[1] Batch[86] Loss: 0.15\n",
      "Epoch[1] Batch[87] Loss: 0.11\n",
      "Epoch[1] Batch[88] Loss: 0.13\n",
      "Epoch[1] Batch[89] Loss: 0.12\n",
      "Epoch[1] Batch[90] Loss: 0.15\n",
      "Epoch[1] Batch[91] Loss: 0.14\n",
      "Epoch[1] Batch[92] Loss: 0.18\n",
      "Epoch[1] Batch[93] Loss: 0.12\n",
      "Epoch[1] Batch[94] Loss: 0.11\n",
      "Epoch[1] Batch[95] Loss: 0.11\n",
      "Epoch[1] Batch[96] Loss: 0.15\n",
      "Epoch[1] Batch[97] Loss: 0.11\n",
      "Epoch[1] Batch[98] Loss: 0.11\n",
      "Epoch[1] Batch[99] Loss: 0.13\n",
      "Epoch[1] Batch[100] Loss: 0.11\n",
      "Epoch[1] Batch[101] Loss: 0.14\n",
      "Epoch[1] Batch[102] Loss: 0.09\n",
      "Epoch[1] Batch[103] Loss: 0.10\n",
      "Epoch[1] Batch[104] Loss: 0.11\n",
      "Epoch[1] Batch[105] Loss: 0.13\n",
      "Epoch[1] Batch[106] Loss: 0.12\n",
      "Epoch[1] Batch[107] Loss: 0.14\n",
      "Epoch[1] Batch[108] Loss: 0.11\n",
      "Epoch[1] Batch[109] Loss: 0.10\n",
      "Epoch[1] Batch[110] Loss: 0.11\n",
      "Epoch[1] Batch[111] Loss: 0.08\n",
      "Epoch[1] Batch[112] Loss: 0.08\n",
      "Epoch[1] Batch[113] Loss: 0.08\n",
      "Epoch[1] Batch[114] Loss: 0.08\n",
      "Epoch[1] Batch[115] Loss: 0.11\n",
      "Epoch[1] Batch[116] Loss: 0.10\n",
      "Epoch[1] Batch[117] Loss: 0.10\n",
      "Epoch[1] Batch[118] Loss: 0.14\n",
      "Epoch[1] Batch[119] Loss: 0.12\n",
      "Epoch[1] Batch[120] Loss: 0.10\n",
      "Epoch[1] Batch[121] Loss: 0.10\n",
      "Epoch[1] Batch[122] Loss: 0.08\n",
      "Epoch[1] Batch[123] Loss: 0.08\n",
      "Epoch[1] Batch[124] Loss: 0.11\n",
      "Epoch[1] Batch[125] Loss: 0.10\n",
      "Epoch[1] Batch[126] Loss: 0.12\n",
      "Epoch[1] Batch[127] Loss: 0.12\n",
      "Epoch[1] Batch[128] Loss: 0.12\n",
      "Epoch[1] Batch[129] Loss: 0.11\n",
      "Epoch[1] Batch[130] Loss: 0.10\n",
      "Epoch[1] Batch[131] Loss: 0.12\n",
      "Epoch[1] Batch[132] Loss: 0.09\n",
      "Epoch[1] Batch[133] Loss: 0.10\n",
      "Epoch[1] Batch[134] Loss: 0.12\n",
      "Epoch[1] Batch[135] Loss: 0.14\n",
      "Epoch[1] Batch[136] Loss: 0.10\n",
      "Epoch[1] Batch[137] Loss: 0.09\n",
      "Epoch[1] Batch[138] Loss: 0.14\n",
      "Epoch[1] Batch[139] Loss: 0.12\n",
      "Epoch[1] Batch[140] Loss: 0.09\n",
      "Epoch[1] Batch[141] Loss: 0.08\n",
      "Epoch[1] Batch[142] Loss: 0.14\n",
      "Epoch[1] Batch[143] Loss: 0.11\n",
      "Epoch[1] Batch[144] Loss: 0.15\n",
      "Epoch[1] Batch[145] Loss: 0.14\n",
      "Epoch[1] Batch[146] Loss: 0.11\n",
      "Epoch[1] Batch[147] Loss: 0.14\n",
      "Epoch[1] Batch[148] Loss: 0.12\n",
      "Epoch[1] Batch[149] Loss: 0.07\n",
      "Epoch[1] Batch[150] Loss: 0.09\n",
      "Epoch[1] Batch[151] Loss: 0.09\n",
      "Epoch[1] Batch[152] Loss: 0.15\n",
      "Epoch[1] Batch[153] Loss: 0.09\n",
      "Epoch[1] Batch[154] Loss: 0.10\n",
      "Epoch[1] Batch[155] Loss: 0.11\n",
      "Epoch[1] Batch[156] Loss: 0.08\n",
      "Epoch[1] Batch[157] Loss: 0.12\n",
      "Epoch[1] Batch[158] Loss: 0.11\n",
      "Epoch[1] Batch[159] Loss: 0.16\n",
      "Epoch[1] Batch[160] Loss: 0.12\n",
      "Epoch[1] Batch[161] Loss: 0.10\n",
      "Epoch[1] Batch[162] Loss: 0.13\n",
      "Epoch[1] Batch[163] Loss: 0.12\n",
      "Epoch[1] Batch[164] Loss: 0.07\n",
      "Epoch[1] Batch[165] Loss: 0.10\n",
      "Epoch[1] Batch[166] Loss: 0.08\n",
      "Epoch[1] Batch[167] Loss: 0.10\n",
      "Epoch[1] Batch[168] Loss: 0.10\n",
      "Epoch[1] Batch[169] Loss: 0.10\n",
      "Epoch[1] Batch[170] Loss: 0.10\n",
      "Epoch[1] Batch[171] Loss: 0.08\n",
      "Epoch[1] Batch[172] Loss: 0.13\n",
      "Epoch[1] Batch[173] Loss: 0.13\n",
      "Epoch[1] Batch[174] Loss: 0.07\n",
      "Epoch[1] Batch[175] Loss: 0.10\n",
      "Epoch[1] Batch[176] Loss: 0.10\n",
      "Epoch[1] Batch[177] Loss: 0.09\n",
      "Epoch[1] Batch[178] Loss: 0.10\n",
      "Epoch[1] Batch[179] Loss: 0.08\n",
      "Epoch[1] Batch[180] Loss: 0.05\n",
      "Training Results   - Epoch: 1  accuracy: 0.97 loss: 0.10 prec: tensor([ 0.9653,  0.9699])\n",
      "Validation Results - Epoch: 1  accuracy: 0.97 loss: 0.08 prec: tensor([ 0.9739,  0.9721])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7fcc486b56d8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results   - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision'].cpu()}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision'].cpu()}\")\n",
    "\n",
    "\n",
    "trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with Tensorboard and TensorboardX\n",
    "- https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard\n",
    "- https://github.com/lanpa/tensorboard-pytorch\n",
    "\n",
    "Start tensorboard:\n",
    "```\n",
    "cd notebooks\n",
    "tensorboard --logdir=tf_log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "summary_writer = SummaryWriter(log_dir=\"tf_log/exp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mexp1\u001b[0m/  \u001b[01;34mexp1_ignite\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls tf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurs, checking if it's onnx problem...\n",
      "Your model fails onnx too, please report to onnx team\n",
      "No graph saved\n"
     ]
    }
   ],
   "source": [
    "# Visualize the graph/network\n",
    "X, _ = next(iter(train_dl))\n",
    "summary_writer.add_graph(model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then visit http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some scalars\n",
    "for i in range(10):\n",
    "    summary_writer.add_scalar(\"training/loss\", np.random.rand(), i)\n",
    "    summary_writer.add_scalar(\"validation/loss\", np.random.rand() + .1, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tensorboard with ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new SummaryWriter for new experiment\n",
    "summary_writer = SummaryWriter(log_dir=\"tf_log/exp1_ignite\")\n",
    "\n",
    "# Basic setup: model, loss, optimizer\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(get_trainable(model.parameters()), lr=0.001, momentum=.9)\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": CategoricalAccuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "    },\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"training/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"training/loss\", metrics['loss'], epoch)\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"validation/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"validation/loss\", metrics['loss'], epoch)\n",
    "    print(metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9735\n",
      "0.975\n",
      "0.9765\n",
      "0.976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7fcc42faeeb8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_dl, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermission: the output of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 0\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Outlook\n",
    "- [0.4 \"just\" released](https://pytorch.org/2018/04/22/0_4_0-migration-guide.html)\n",
    "  - `Tensors` and `Variables` have merged\n",
    "  - Support for 0-dimensional (scalar) Tensors\n",
    "  - Deprecation of the `volatile` flag\n",
    "  \n",
    "- [The road to 1.0: production ready PyTorch](https://pytorch.org/2018/05/02/road-to-1.0.html)\n",
    "  - `torch.jit`\n",
    "  - optimize for mobile\n",
    "  - quantized inference (such as 8-bit inference)\n",
    "  - caffe2 merget into pytorch repo\n",
    "  - ONNX - Open Neural Network Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\nunknown builtin op:\n@script\ndef rnn_loop(x):\n    hidden = None\n    for x_t in x.split(1):\n               ~~~~~~~ <--- HERE\n        x, hidden = model(x, hidden)\n    return x\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5ac59a41fce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrnn_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pydata_pytorch_tutorial/lib/python3.6/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mscript\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_script_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pydata_pytorch_tutorial/lib/python3.6/site-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36m_script_graph\u001b[0;34m(fn, frame_id)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mrcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateResolutionCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0mast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_jit_ast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_jit_script_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \nunknown builtin op:\n@script\ndef rnn_loop(x):\n    hidden = None\n    for x_t in x.split(1):\n               ~~~~~~~ <--- HERE\n        x, hidden = model(x, hidden)\n    return x\n"
     ]
    }
   ],
   "source": [
    "from torch.jit import script\n",
    "\n",
    "@script\n",
    "def rnn_loop(x):\n",
    "    hidden = None\n",
    "    for x_t in x.split(1):\n",
    "        x, hidden = model(x, hidden)\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
