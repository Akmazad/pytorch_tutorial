{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning with PyTorch\n",
    "We're going to train a neural network to classify dogs and cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init, helpers, utils, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attr(obj):\n",
    "    \"\"\"\n",
    "    Return all public attributes of an object.\n",
    "    \"\"\"\n",
    "    return [x for x in dir(obj) if not x.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helpers\n",
    "\n",
    "def get_trainable(model_params):\n",
    "    return (p for p in model_params if p.requires_grad)\n",
    "\n",
    "\n",
    "def get_frozen(model_params):\n",
    "    return (p for p in model_params if not p.requires_grad)\n",
    "\n",
    "\n",
    "def all_trainable(model_params):\n",
    "    return all(p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def all_frozen(model_params):\n",
    "    return all(not p.requires_grad for p in model_params)\n",
    "\n",
    "\n",
    "def freeze_all(model_params):\n",
    "    for param in model_params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# list(get_trainable(model.parameters()))\n",
    "# list(get_frozen(model.parameters()))\n",
    "# all_trainable(model.parameters())\n",
    "# all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "IMG_SIZE = 224\n",
    "_mean = [0.485, 0.456, 0.406]\n",
    "_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "\n",
    "train_trans = transforms.Compose([\n",
    "    transforms.Resize(256),  # some images are pretty small\n",
    "    transforms.RandomCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(.3, .3, .3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(_mean, _std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/raw/dogscats/sample/train.\n",
      "Loading data from ../data/raw/dogscats/sample/valid.\n"
     ]
    }
   ],
   "source": [
    "from ppt.utils import DogsCatsDataset\n",
    "\n",
    "train_ds = DogsCatsDataset(\"../data/raw\", \"sample/train\", transform=train_trans)\n",
    "val_ds = DogsCatsDataset(\"../data/raw\", \"sample/valid\", transform=val_trans)\n",
    "\n",
    "n_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "Batch loading for datasets with multi-processing and different sample strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_all(model.parameters())\n",
    "assert all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the last layer with a linear layer. New layers have `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_frozen(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(n_classes=2):\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    freeze_all(model.parameters())\n",
    "    model.fc = nn.Linear(512, n_classes)\n",
    "    return model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    get_trainable(model.parameters()),  # model.fc.parameters()\n",
    "    lr=0.0001,\n",
    "    momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "    loss: 0.588\n",
      "    loss: 0.955\n",
      "    loss: 0.660\n",
      "    loss: 0.928\n",
      "    loss: 0.489\n",
      "    loss: 0.593\n",
      "    loss: 0.700\n",
      "    loss: 0.713\n",
      "  Train Loss: 0.7033335417509079\n",
      "  Train Acc:  0.625\n",
      "  Valid Loss: 0.7131053805351257\n",
      "  Valid Acc:  1.0\n",
      "\n",
      "Epoch 1/2\n",
      "    loss: 0.643\n",
      "    loss: 0.771\n",
      "    loss: 0.571\n",
      "    loss: 0.414\n",
      "    loss: 0.687\n",
      "    loss: 0.606\n",
      "    loss: 0.768\n",
      "    loss: 0.355\n",
      "  Train Loss: 0.6016690582036972\n",
      "  Train Acc:  0.75\n",
      "  Valid Loss: 0.3547409772872925\n",
      "  Valid Acc:  1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch}/{N_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    model.train()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    \n",
    "    for X, y in train_dl:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_ = model(X)\n",
    "        loss = criterion(y_, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"    loss: {loss.item():0.3f}\")\n",
    "        \n",
    "        _, y_label_ = torch.max(y_, 1)\n",
    "        correct += (y_label_ == y).sum().item()\n",
    "        running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Train Loss: {running_loss / len(train_dl.dataset)}\")\n",
    "    print(f\"  Train Acc:  {correct / len(train_dl.dataset)}\")\n",
    "    \n",
    "    \n",
    "    # Eval\n",
    "    model.eval()  # IMPORTANT\n",
    "    \n",
    "    running_loss, correct = 0.0, 0\n",
    "    \n",
    "    with torch.no_grad():  # IMPORTANT\n",
    "        for X, y in val_dl:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_ = model(X)\n",
    "            lossl = criterion(y_, y)\n",
    "        \n",
    "            _, y_label_ = torch.max(y_, 1)\n",
    "            correct += (y_label_ == y).sum().item()\n",
    "            running_loss += loss.item() * X.shape[0]\n",
    "    \n",
    "    print(f\"  Valid Loss: {running_loss / len(val_dl.dataset)}\")\n",
    "    print(f\"  Valid Acc:  {correct / len(val_dl.dataset)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermission: training libraries\n",
    "\n",
    "### Ignite\n",
    "Ignite is a high-level library to help with training neural networks in PyTorch.\n",
    "\n",
    "- ignite helps you write compact but full-featured training loops in a few lines of code\n",
    "- you get a training loop with metrics, early-stopping, model checkpointing and other features without the boilerplate\n",
    "\n",
    "https://github.com/pytorch/ignite\n",
    "\n",
    "### TNT\n",
    "TNT is a library providing powerful dataloading, logging and visualization utlities for Python. It is closely intergrated with PyTorch and is designed to enable rapid iteration with any model or training regimen.\n",
    "\n",
    "https://github.com/pytorch/tnt\n",
    "\n",
    "\n",
    "### Skorch\n",
    "A scikit-learn compatible neural network library that wraps PyTorch.\n",
    "\n",
    "https://github.com/dnouri/skorch\n",
    "\n",
    "### The fun of Reinvention\n",
    "Clearly, you can do better than they, invent your own! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ignite\n",
    "from ignite.metrics import (\n",
    "    CategoricalAccuracy,\n",
    "    Loss,\n",
    "    Precision,\n",
    ")\n",
    "from ignite.engine import (\n",
    "    create_supervised_evaluator,\n",
    "    create_supervised_trainer,\n",
    "    Events,\n",
    ")\n",
    "\n",
    "# model, loss, optimizer\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(get_trainable(model.parameters()), lr=0.001, momentum=.9)\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": CategoricalAccuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "        \"precision\": Precision(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Batch[1] Loss: 0.92\n",
      "Epoch[1] Batch[2] Loss: 0.64\n",
      "Epoch[1] Batch[3] Loss: 0.81\n",
      "Epoch[1] Batch[4] Loss: 0.39\n",
      "Epoch[1] Batch[5] Loss: 0.82\n",
      "Epoch[1] Batch[6] Loss: 1.76\n",
      "Epoch[1] Batch[7] Loss: 0.86\n",
      "Epoch[1] Batch[8] Loss: 0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7fedf8f4df28>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    print(f\"Epoch[{engine.state.epoch}] Batch[{engine.state.iteration}] Loss: {engine.state.output:.2f}\")\n",
    "\n",
    "\n",
    "trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1] Loss: 0.12\n",
      "Epoch[1] Loss: 0.12\n",
      "Epoch[1] Loss: 0.13\n",
      "Epoch[1] Loss: 0.13\n",
      "Epoch[1] Loss: 2.26\n",
      "Epoch[1] Loss: 2.26\n",
      "Epoch[1] Loss: 1.81\n",
      "Epoch[1] Loss: 1.81\n",
      "Epoch[1] Loss: 0.19\n",
      "Epoch[1] Loss: 0.19\n",
      "Epoch[1] Loss: 0.38\n",
      "Epoch[1] Loss: 0.38\n",
      "Epoch[1] Loss: 2.44\n",
      "Epoch[1] Loss: 2.44\n",
      "Epoch[1] Loss: 0.62\n",
      "Epoch[1] Loss: 0.62\n",
      "Training Results   - Epoch: 1  accuracy: 0.94 loss: 0.11 prec: tensor([ 1.0000,  0.8889])\n",
      "Validation Results - Epoch: 1  accuracy: 0.62 loss: 0.54 prec: tensor([ 1.0000,  0.5714])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7fee0cdca4e0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Training Results   - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision']}\")\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    print(f\"Validation Results - Epoch: {trainer.state.epoch}  \"\n",
    "          f\"accuracy: {metrics['accuracy']:.2f} \"\n",
    "          f\"loss: {metrics['loss']:.2f} \"\n",
    "          f\"prec: {metrics['precision']}\")\n",
    "\n",
    "\n",
    "trainer.run(train_dl, max_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with Tensorboard and TensorboardX\n",
    "- https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard\n",
    "- https://github.com/lanpa/tensorboard-pytorch\n",
    "\n",
    "Start tensorboard:\n",
    "```\n",
    "cd notebooks\n",
    "tensorboard --logdir=tf_log\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "summary_writer = SummaryWriter(log_dir=\"tf_log/exp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mexp1\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls tf_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph/network\n",
    "X, _ = next(iter(train_dl))\n",
    "summary_writer.add_graph(model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then visit http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write some scalars\n",
    "for i in range(10):\n",
    "    summary_writer.add_scalar(\"training/loss\", np.random.rand(), i)\n",
    "    summary_writer.add_scalar(\"validation/loss\", np.random.rand() + .1, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tensorboard with ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new SummaryWriter for new experiment\n",
    "summary_writer = SummaryWriter(log_dir=\"tf_log/exp1_ignite\")\n",
    "\n",
    "# Basic setup: model, loss, optimizer\n",
    "model = get_model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(get_trainable(model.parameters()), lr=0.001, momentum=.9)\n",
    "\n",
    "# trainer and evaluator\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion)\n",
    "evaluator = create_supervised_evaluator(\n",
    "    model,\n",
    "    metrics={\n",
    "        \"accuracy\": CategoricalAccuracy(),\n",
    "        \"loss\": Loss(criterion),\n",
    "    }\n",
    ")\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_training_results(trainer):\n",
    "    evaluator.run(train_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"training/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"training/loss\", metrics['loss'], epoch)\n",
    "\n",
    "\n",
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def log_validation_results(engine):\n",
    "    evaluator.run(val_dl)\n",
    "    metrics = evaluator.state.metrics\n",
    "    epoch = trainer.state.epoch\n",
    "    summary_writer.add_scalar(\"validation/accuracy\", metrics['accuracy'], epoch)\n",
    "    summary_writer.add_scalar(\"validation/loss\", metrics['loss'], epoch)\n",
    "    print(metrics['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ignite.engine.engine.State at 0x7fee00372da0>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run(train_dl, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermission: the output of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(y_, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write your own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 0\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with PyTorch\n",
    "# Simple Neural Network\n",
    "# Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detour: K-Means in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Outlook\n",
    "- [0.4 \"just\" released](https://pytorch.org/2018/04/22/0_4_0-migration-guide.html)\n",
    "  - `Tensors` and `Variables` have merged\n",
    "  - Support for 0-dimensional (scalar) Tensors\n",
    "  - Deprecation of the `volatile` flag\n",
    "  \n",
    "- [The road to 1.0: production ready PyTorch](https://pytorch.org/2018/05/02/road-to-1.0.html)\n",
    "  - `torch.jit`\n",
    "  - optimize for mobile\n",
    "  - quantized inference (such as 8-bit inference)\n",
    "  - caffe2 merget into pytorch repo\n",
    "  - ONNX - Open Neural Network Exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.jit import script\n",
    "\n",
    "@script\n",
    "def rnn_loop(x):\n",
    "    hidden = None\n",
    "    for x_t in x.split(1):\n",
    "        x, hidden = model(x, hidden)\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
